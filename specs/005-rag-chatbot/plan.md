# Implementation Plan: Integrated RAG Chatbot

**Branch**: `005-rag-chatbot` | **Date**: 2025-12-04 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/005-rag-chatbot/spec.md`

## Summary

Build and integrate a Retrieval-Augmented Generation (RAG) chatbot within the Physical AI and Humanoid Robotics Docusaurus book website. The system uses Hugging Face sentence-transformers for local embedding generation, Qdrant for vector storage, OpenAI Agents Python SDK for conversation management, and Neon Serverless Postgres for session/feedback storage. Key features include full-book semantic search, text selection mode, conversation history, suggested questions, and user feedback mechanisms.

**Technical Approach**:
- **Embeddings**: Local sentence-transformers model (BAAI/bge-base-en-v1.5) generates 768-dimensional vectors, eliminating OpenAI embedding API costs
- **Agent Framework**: OpenAI Agents Python SDK provides conversation management and response generation
- **Database**: Neon Serverless Postgres with automatic connection pooling for serverless environments
- **Vector DB**: Qdrant Cloud for semantic search with cosine similarity

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**:
- Backend: FastAPI, OpenAI Agents Python SDK, sentence-transformers, qdrant-client, SQLAlchemy, psycopg, Pydantic, slowapi
- Frontend: React, react-markdown, Docusaurus 3.x

**Storage**:
- Vector DB: Qdrant Cloud (1GB free tier)
- Relational DB: Neon Serverless Postgres (0.5GB free tier)
- Session data: PostgreSQL with automatic connection pooling

**Testing**: pytest, pytest-asyncio, pytest-cov, httpx (async testing)

**Target Platform**:
- Backend: Render free-tier web service (Linux server)
- Frontend: GitHub Pages (static site)

**Project Type**: Web application (backend API + frontend plugin)

**Performance Goals**:
- p95 response time <3 seconds (end-to-end)
- Support 50 concurrent chat sessions
- Embedding generation: batch processing for efficiency
- Incremental re-ingestion: <2 minutes per chapter

**Constraints**:
- Free-tier infrastructure: $0 (Qdrant, Neon, Render) + ~$10-15/month (OpenAI Agents API)
- Anonymous users only (no authentication)
- English only (no i18n/l10n)
- Desktop-first (mobile secondary)

**Scale/Scope**:
- Content: 12+ book chapters → 500+ vector chunks
- Users: Designed for educational website traffic (concurrent sessions <50)
- Storage: <1GB vector data, <500MB relational data

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

✅ **Code Quality**:
- Type hints (Python 3.11+ typing)
- Pydantic models for validation
- Backend unit test coverage ≥80%
- Frontend component test coverage ≥70%

✅ **Performance**:
- Batch embedding generation (sentence-transformers)
- Connection pooling (Neon's automatic PgBouncer)
- Vector search optimization (Qdrant payload indices)
- p95 latency <3s enforced

✅ **Security**:
- Rate limiting: 10 requests/minute per IP (slowapi)
- Input validation: Pydantic schemas
- No API key exposure in frontend
- Generic error messages (no internal leakage)

✅ **Architecture**:
- Service layer separation (RAG, OpenAI, Qdrant, Database)
- Dependency injection (FastAPI)
- Clean separation: ingestion scripts, API routers, services

## Project Structure

### Documentation (this feature)

```text
specs/005-rag-chatbot/
├── spec.md              # Feature requirements and user stories
├── plan.md              # This file - implementation plan
├── tasks.md             # Task breakdown (generated by /sp.tasks)
├── research.md          # Technology evaluation and selection
├── data-model.md        # Database schemas and entity relationships
├── quickstart.md        # Developer setup guide
└── contracts/           # API contracts and schemas
    └── openapi.yaml     # REST API specification
```

### Source Code (repository root)

```text
backend/
├── main.py              # FastAPI application entry point
├── config.py            # Settings management (pydantic-settings)
├── models/
│   ├── database.py      # SQLAlchemy ORM models
│   └── schemas.py       # Pydantic request/response schemas
├── services/
│   ├── embedding_service.py     # Sentence-transformers wrapper
│   ├── agent_service.py         # OpenAI Agents SDK wrapper
│   ├── qdrant_service.py        # Qdrant vector DB operations
│   ├── rag_service.py           # RAG orchestration
│   └── database_service.py      # Session/feedback CRUD
├── routers/
│   ├── chat.py          # POST /api/chat endpoint
│   ├── session.py       # POST /api/session endpoint
│   ├── feedback.py      # POST /api/feedback endpoint
│   └── health.py        # GET /health endpoint
├── scripts/
│   ├── ingest_content.py       # Content ingestion CLI
│   └── verify_chunks.py        # Validation script
├── tests/
│   ├── test_embedding_service.py
│   ├── test_agent_service.py
│   ├── test_qdrant_service.py
│   ├── test_rag_service.py
│   └── test_api_endpoints.py
├── alembic/             # Database migrations
│   ├── env.py
│   └── versions/
├── requirements.txt     # Python dependencies
└── .env.example         # Environment variable template

book_frontend/
├── docs/                # Markdown content (ingestion source)
├── src/
│   └── plugins/
│       └── docusaurus-plugin-rag-chatbot/
│           ├── index.js          # Plugin entry point
│           ├── components/
│           │   ├── ChatWidget.tsx      # Main chat UI
│           │   ├── ChatInput.tsx       # User input component
│           │   ├── ChatMessage.tsx     # Message display
│           │   ├── ChatHistory.tsx     # Conversation history
│           │   └── FeedbackButtons.tsx # Rating UI
│           └── hooks/
│               ├── useChat.ts          # Chat state management
│               └── useSession.ts       # Session persistence
├── package.json
└── docusaurus.config.js  # Plugin registration
```

**Structure Decision**: Web application structure chosen because the feature requires both a backend API (FastAPI) and a frontend plugin (Docusaurus/React). Backend handles RAG operations, database, and OpenAI Agents integration. Frontend provides chat UI integrated into the Docusaurus book site.

## Architectural Decisions

### ADR-001: Hugging Face Sentence-Transformers for Embeddings

**Context**: Need to generate semantic embeddings for book content and user queries.

**Decision**: Use Hugging Face sentence-transformers library with `BAAI/bge-base-en-v1.5` model.

**Alternatives Considered**:
1. **OpenAI text-embedding-3-small**:
   - ❌ Cost: $0.02 per 1M tokens (adds recurring cost)
   - ❌ Latency: Network round-trip for every embedding
   - ✅ Quality: High-quality embeddings, 1536 dimensions

2. **Sentence-transformers (chosen)**:
   - ✅ Cost: Free (runs locally)
   - ✅ Latency: Fast batch encoding (no network)
   - ✅ Quality: BGE models optimized for retrieval (768 dimensions)
   - ✅ Open source: Full control, no vendor lock-in

**Rationale**:
- BGE models (Beijing Academy of AI) rank top on MTEB leaderboard for retrieval tasks
- 768 dimensions provide excellent quality-to-performance ratio
- Local execution eliminates API costs and network latency
- Batch encoding (`model.encode()`) efficiently processes ingestion workload
- Free-tier compliance: $0 embedding cost

**Implementation**:
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-base-en-v1.5")
embeddings = model.encode(texts, batch_size=50)  # 768-dim vectors
```

**Trade-offs**:
- Model download: ~150MB (one-time, cached)
- Memory: ~500MB model footprint (acceptable for Render)
- Dimension reduction: 768 vs 1536 (minimal quality impact for this use case)

---

### ADR-002: OpenAI Agents Python SDK for Conversation Management

**Context**: Need to generate contextual responses using LLM with conversation management.

**Decision**: Use OpenAI Agents Python SDK with `Agent` and `Runner` classes for response generation.

**Alternatives Considered**:
1. **OpenAI Chat Completions API (direct)**:
   - ✅ Simple: Direct API calls
   - ❌ Manual: No built-in conversation management
   - ❌ Boilerplate: Custom session handling, context tracking

2. **LangChain**:
   - ✅ Feature-rich: Many integrations
   - ❌ Heavy: Large dependency footprint
   - ❌ Complexity: Over-engineered for simple RAG use case

3. **OpenAI Agents Python SDK (chosen)**:
   - ✅ Official: First-party SDK from OpenAI
   - ✅ Lightweight: Focused on agent orchestration
   - ✅ Built-in: Session management, conversation context
   - ✅ Async: Native async/await support

**Rationale**:
- Provides `Agent` class for defining behavior and `Runner` for execution
- Handles conversation context and session persistence automatically
- Async iteration for streaming responses (optional, post-MVP)
- Usage tracking built-in (token counts for cost monitoring)
- Simpler than LangChain for single-agent RAG use case

**Implementation**:
```python
from agents import Agent, Runner, ModelSettings

agent = Agent(
    name="RAG Assistant",
    instructions="Answer questions about Physical AI book...",
    model_settings=ModelSettings(
        model="gpt-4o-mini",
        temperature=0.3,
        max_tokens=1000
    )
)

result = await Runner.run(agent, context_messages)
```

**Trade-offs**:
- Vendor lock-in: Tied to OpenAI SDK (mitigated by abstraction layer)
- Model cost: ~$0.15/1M input tokens, ~$0.60/1M output tokens
- No multi-agent needed: Single-agent pattern sufficient for this use case

---

### ADR-003: Neon Serverless Postgres for Session/Feedback Storage

**Context**: Need relational database for session management, user queries, responses, and feedback.

**Decision**: Use Neon Serverless Postgres with automatic connection pooling.

**Alternatives Considered**:
1. **Traditional PostgreSQL (self-hosted)**:
   - ❌ Ops burden: Manual scaling, backups, monitoring
   - ❌ Cost: Server hosting even at zero load
   - ✅ Control: Full configuration access

2. **SQLite**:
   - ✅ Simple: Single-file database
   - ❌ Concurrency: Limited write concurrency
   - ❌ Serverless: Not suitable for distributed API deployment

3. **Neon Serverless Postgres (chosen)**:
   - ✅ Serverless: Auto-scaling, pay-per-use
   - ✅ Free tier: 0.5GB storage, sufficient for MVP
   - ✅ Connection pooling: Built-in PgBouncer (critical for serverless)
   - ✅ PostgreSQL: Full compatibility with SQLAlchemy

**Rationale**:
- Connection pooling essential for serverless environments (Render)
- Neon automatically provides pooled connection string (`-pooler` suffix)
- Scales to zero when inactive (free-tier friendly)
- PostgreSQL compatibility enables rich queries, JSONB, and constraints
- No infrastructure management required

**Implementation**:
```python
# Connection string with pooling
DATABASE_URL = "postgresql://user:pass@ep-xxx-pooler.neon.tech/db"

engine = create_engine(
    DATABASE_URL,
    pool_pre_ping=True,
    pool_size=5,
    max_overflow=10
)
```

**Trade-offs**:
- Free tier limits: 0.5GB storage, 100 compute hours/month (sufficient for MVP)
- Cold starts: Initial query may have ~100ms delay (acceptable)
- Regional: Single-region deployment (not multi-region)

---

### ADR-004: Qdrant Cloud for Vector Storage

**Context**: Need vector database for semantic search over book content embeddings.

**Decision**: Use Qdrant Cloud free tier with 768-dimensional vectors.

**Alternatives Considered**:
1. **Pinecone**:
   - ✅ Managed: Serverless vector DB
   - ❌ Free tier: Limited to 1GB, 5M vectors
   - ❌ Vendor-specific: Less open ecosystem

2. **PostgreSQL with pgvector**:
   - ✅ Unified: Single database for vectors + relational
   - ❌ Performance: Not optimized for large-scale vector search
   - ❌ Complexity: Manual index tuning required

3. **Qdrant Cloud (chosen)**:
   - ✅ Free tier: 1GB storage (sufficient for 500+ chunks)
   - ✅ Performance: Purpose-built for vector search
   - ✅ Open source: Self-hostable if needed
   - ✅ Filtering: Payload-based filtering (e.g., by module)

**Rationale**:
- Optimized for cosine similarity search with HNSW indexing
- Payload support enables metadata filtering (module, chapter)
- REST API with Python client (qdrant-client)
- Free tier sufficient for educational book content (~500-1000 chunks)
- Open source core provides exit strategy

**Implementation**:
```python
from qdrant_client import QdrantClient

client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

# Create collection with 768 dimensions (BGE model)
client.create_collection(
    collection_name="book_content",
    vectors_config={"size": 768, "distance": "Cosine"}
)

# Search with metadata filtering
results = client.search(
    collection_name="book_content",
    query_vector=query_embedding,
    limit=10,
    score_threshold=0.7,
    query_filter={"module_number": 1}  # Optional filtering
)
```

**Trade-offs**:
- Separate service: Two databases to manage (Qdrant + Postgres)
- Free tier: 1GB limit (adequate for current scope, may need upgrade for multi-book)
- Network dependency: Cloud-hosted (requires internet)

---

### ADR-005: FastAPI for Backend API

**Context**: Need REST API framework for chat endpoints, session management, and feedback collection.

**Decision**: Use FastAPI with async/await for all I/O operations.

**Rationale**:
- Native async support (critical for OpenAI Agents SDK, Qdrant, Postgres)
- Automatic OpenAPI documentation (self-documenting API)
- Pydantic integration for request/response validation
- High performance (Starlette + Uvicorn)
- Pythonic: Type hints, dependency injection

**No alternatives considered**: FastAPI is industry standard for modern Python APIs and fits requirements perfectly.

---

### ADR-006: Docusaurus Plugin for Frontend Integration

**Context**: Need to integrate chat UI into existing Docusaurus book website.

**Decision**: Build custom Docusaurus plugin with React components for chat widget.

**Rationale**:
- Docusaurus plugin architecture enables site-wide integration
- React components reusable across all book pages
- Keyboard shortcuts (Ctrl+K) for consistent UX
- Session persistence via localStorage
- Text selection mode via browser Selection API

**Implementation**:
```typescript
// Plugin provides <ChatWidget /> component globally
export default function RAGChatbotPlugin(context, options) {
  return {
    name: 'docusaurus-plugin-rag-chatbot',
    getClientModules() {
      return [require.resolve('./ChatWidget')];
    }
  };
}
```

**Trade-offs**:
- Docusaurus-specific: Not portable to other static site generators
- React only: Cannot support non-React Docusaurus themes
- Client-side: All chat logic runs in browser (acceptable for this use case)

---

## Implementation Phases

See `tasks.md` for detailed task breakdown. High-level phases:

1. **Phase 1**: Project Setup & Infrastructure (Qdrant, Neon, environment)
2. **Phase 2**: Foundational Backend Services (embedding, vector DB, database)
3. **Phase 3**: US1 - Full Book Search (MVP milestone: core RAG chatbot)
4. **Phase 4**: US2 - Selection Mode
5. **Phase 5**: US3 - Suggested Questions
6. **Phase 6**: US4 - Conversation History
7. **Phase 7**: US5 - Feedback System
8. **Phase 8**: Deployment & Monitoring

**MVP Scope**: Phases 1-3 (core RAG chatbot with full-book search)

---

## Risk Analysis

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Free-tier limits exceeded | Medium | High | Monitor usage, implement rate limiting, have paid tier upgrade path |
| Embedding model size (150MB) | Low | Low | Model cached after first load, acceptable for Render |
| Qdrant cold starts | Low | Medium | Keep-alive pings, upgrade to paid tier if needed |
| Neon connection pool exhaustion | Low | High | Use Neon's pooled connection string, set max pool size |
| OpenAI API rate limits | Low | Medium | Implement exponential backoff, queue requests |
| Content ingestion time | Low | Low | Batch processing optimized, <5 minutes for full book |

---

## Success Metrics

- ✅ p95 response time <3 seconds
- ✅ 500+ book chunks ingested and searchable
- ✅ 90%+ citation accuracy (links navigate to correct chapters)
- ✅ Free-tier compliance ($0 infrastructure + ~$10-15 OpenAI)
- ✅ 80%+ backend test coverage
- ✅ Zero API key exposure in client

---

## Next Steps

1. Run `/sp.tasks` to generate detailed task breakdown from this plan
2. Execute tasks following red-green-refactor cycle
3. Create PHR (Prompt History Record) after each major phase
4. Document ADRs using `/sp.adr` for any additional architectural decisions

---

**Plan Date**: 2025-12-04
**Last Updated**: 2025-12-04
**Status**: Ready for task generation
